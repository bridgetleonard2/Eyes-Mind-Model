{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin.index.json: 100%|██████████| 33.7k/33.7k [00:00<00:00, 2.45MB/s]\n",
      "c:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bridget Leonard\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [20:34<00:00, 8.06MB/s]\n",
      "pytorch_model-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [06:09<00:00, 26.8MB/s]\n",
      "pytorch_model-00003-of-00003.bin: 100%|██████████| 6.24G/6.24G [03:06<00:00, 33.4MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [29:53<00:00, 597.85s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:19<00:00, 26.43s/it]\n",
      "Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at liuhaotian/llava-v1.5-13b and are newly initialized: ['model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.language_model.model.layers.5.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.post_layernorm.bias', 'model.language_model.model.layers.15.mlp.up_proj.weight', 'model.language_model.model.layers.1.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.language_model.model.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.language_model.model.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.language_model.model.layers.13.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.language_model.model.layers.3.post_attention_layernorm.weight', 'model.language_model.model.layers.15.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.language_model.model.layers.31.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.language_model.model.layers.27.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.language_model.model.layers.1.self_attn.k_proj.weight', 'model.language_model.model.layers.8.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.language_model.model.layers.25.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.language_model.model.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.language_model.model.layers.4.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.language_model.model.layers.6.post_attention_layernorm.weight', 'model.language_model.model.layers.26.post_attention_layernorm.weight', 'model.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.language_model.model.layers.10.mlp.down_proj.weight', 'model.language_model.model.layers.20.self_attn.o_proj.weight', 'model.language_model.model.layers.24.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.language_model.model.layers.22.self_attn.k_proj.weight', 'model.language_model.model.layers.11.input_layernorm.weight', 'model.language_model.model.layers.4.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.language_model.model.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.language_model.model.layers.23.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.language_model.model.layers.5.self_attn.o_proj.weight', 'model.language_model.model.layers.5.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.language_model.model.layers.6.mlp.gate_proj.weight', 'model.language_model.model.layers.19.mlp.gate_proj.weight', 'model.language_model.model.layers.19.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.pre_layrnorm.bias', 'model.language_model.model.layers.29.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.language_model.model.layers.31.mlp.down_proj.weight', 'model.language_model.model.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.language_model.model.layers.10.self_attn.k_proj.weight', 'model.language_model.model.layers.22.self_attn.v_proj.weight', 'model.language_model.model.layers.31.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.language_model.model.layers.12.self_attn.o_proj.weight', 'model.language_model.model.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.language_model.model.layers.23.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.language_model.model.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.multi_modal_projector.linear_2.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.language_model.model.layers.3.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.language_model.model.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.language_model.model.layers.7.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.language_model.model.layers.27.self_attn.q_proj.weight', 'model.language_model.model.layers.20.self_attn.k_proj.weight', 'model.language_model.model.layers.31.mlp.up_proj.weight', 'model.language_model.model.layers.13.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.language_model.model.layers.0.mlp.down_proj.weight', 'model.language_model.model.layers.17.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.language_model.model.layers.17.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.language_model.model.layers.28.self_attn.v_proj.weight', 'model.language_model.model.layers.28.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.language_model.model.layers.22.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.embeddings.class_embedding', 'model.language_model.model.layers.5.mlp.down_proj.weight', 'model.language_model.model.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.language_model.model.layers.15.mlp.gate_proj.weight', 'model.language_model.model.layers.8.self_attn.v_proj.weight', 'model.language_model.model.layers.28.mlp.down_proj.weight', 'model.language_model.model.layers.3.self_attn.k_proj.weight', 'model.language_model.model.layers.17.mlp.down_proj.weight', 'model.language_model.model.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.language_model.model.layers.21.input_layernorm.weight', 'model.language_model.model.layers.29.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.language_model.model.layers.10.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.language_model.model.layers.11.mlp.gate_proj.weight', 'model.language_model.model.layers.21.self_attn.k_proj.weight', 'model.language_model.model.layers.10.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.language_model.model.layers.22.mlp.down_proj.weight', 'model.language_model.model.layers.24.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.language_model.model.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.language_model.model.layers.30.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.language_model.model.layers.13.mlp.down_proj.weight', 'model.language_model.model.layers.7.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.language_model.model.layers.7.mlp.up_proj.weight', 'model.language_model.model.layers.13.self_attn.q_proj.weight', 'model.language_model.model.layers.0.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.language_model.model.layers.22.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.language_model.model.layers.16.self_attn.o_proj.weight', 'model.language_model.model.layers.23.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.language_model.model.layers.20.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.language_model.model.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.language_model.model.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.language_model.model.layers.1.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.language_model.model.layers.6.mlp.down_proj.weight', 'model.language_model.model.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.language_model.model.layers.11.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.language_model.model.layers.6.self_attn.v_proj.weight', 'model.language_model.model.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.language_model.model.layers.14.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.language_model.model.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.language_model.model.layers.9.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.language_model.model.layers.31.mlp.gate_proj.weight', 'model.language_model.model.layers.20.mlp.down_proj.weight', 'model.multi_modal_projector.linear_1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.language_model.model.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.language_model.model.layers.6.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.language_model.model.layers.22.self_attn.q_proj.weight', 'model.language_model.model.layers.29.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.language_model.model.layers.25.mlp.gate_proj.weight', 'model.language_model.model.layers.29.self_attn.v_proj.weight', 'model.language_model.model.layers.29.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.language_model.model.layers.22.input_layernorm.weight', 'model.language_model.model.layers.18.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.language_model.model.layers.18.post_attention_layernorm.weight', 'model.language_model.model.layers.14.mlp.up_proj.weight', 'model.language_model.model.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.language_model.model.layers.28.self_attn.o_proj.weight', 'model.language_model.model.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.language_model.model.layers.24.post_attention_layernorm.weight', 'model.language_model.model.layers.26.self_attn.q_proj.weight', 'model.language_model.model.layers.17.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.language_model.model.layers.3.mlp.down_proj.weight', 'model.language_model.model.layers.15.self_attn.q_proj.weight', 'model.language_model.model.layers.23.self_attn.q_proj.weight', 'model.language_model.model.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.language_model.model.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.language_model.model.layers.29.mlp.down_proj.weight', 'model.language_model.model.layers.9.input_layernorm.weight', 'model.language_model.model.layers.23.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.language_model.model.layers.9.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.language_model.model.layers.23.self_attn.k_proj.weight', 'model.language_model.model.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.language_model.model.layers.8.mlp.gate_proj.weight', 'model.language_model.model.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.language_model.model.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.language_model.model.layers.26.self_attn.o_proj.weight', 'model.language_model.model.layers.19.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.language_model.model.layers.7.self_attn.v_proj.weight', 'model.language_model.model.layers.18.mlp.down_proj.weight', 'model.language_model.model.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.language_model.model.layers.26.self_attn.k_proj.weight', 'model.language_model.model.layers.6.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.language_model.model.layers.4.self_attn.v_proj.weight', 'model.language_model.model.layers.29.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.language_model.model.layers.10.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.language_model.model.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.language_model.model.layers.10.mlp.gate_proj.weight', 'model.language_model.model.layers.7.self_attn.k_proj.weight', 'model.language_model.model.layers.2.mlp.down_proj.weight', 'model.language_model.model.layers.20.mlp.gate_proj.weight', 'model.language_model.model.layers.12.mlp.down_proj.weight', 'model.language_model.model.layers.31.self_attn.q_proj.weight', 'model.language_model.model.layers.17.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.language_model.model.layers.18.self_attn.q_proj.weight', 'model.language_model.model.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.language_model.model.layers.26.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.language_model.model.layers.19.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.language_model.model.layers.12.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.language_model.model.layers.20.self_attn.v_proj.weight', 'model.language_model.model.layers.4.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.language_model.model.layers.24.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.language_model.model.layers.2.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.language_model.model.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.language_model.model.layers.30.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.language_model.model.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.language_model.model.layers.21.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.language_model.model.layers.14.self_attn.o_proj.weight', 'model.language_model.model.layers.16.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.language_model.model.layers.28.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.language_model.model.layers.13.self_attn.v_proj.weight', 'model.language_model.model.layers.15.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.language_model.model.layers.0.self_attn.q_proj.weight', 'model.language_model.model.layers.9.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.language_model.model.layers.8.post_attention_layernorm.weight', 'model.language_model.model.layers.7.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.language_model.model.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.language_model.model.layers.13.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.language_model.model.layers.22.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.language_model.model.layers.30.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.language_model.model.layers.0.mlp.up_proj.weight', 'model.language_model.model.layers.24.self_attn.v_proj.weight', 'model.language_model.model.layers.21.post_attention_layernorm.weight', 'model.language_model.model.layers.14.mlp.down_proj.weight', 'model.language_model.model.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.language_model.model.layers.2.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.language_model.model.layers.18.self_attn.o_proj.weight', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.language_model.model.layers.22.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.language_model.model.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.language_model.model.layers.12.post_attention_layernorm.weight', 'model.language_model.model.layers.25.self_attn.o_proj.weight', 'model.language_model.model.layers.29.self_attn.k_proj.weight', 'model.language_model.model.layers.1.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.language_model.model.layers.3.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.language_model.model.layers.15.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.language_model.model.layers.3.mlp.up_proj.weight', 'model.language_model.model.layers.28.self_attn.k_proj.weight', 'model.language_model.model.layers.12.mlp.gate_proj.weight', 'model.language_model.model.layers.19.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.language_model.model.layers.17.self_attn.q_proj.weight', 'model.language_model.model.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.language_model.model.layers.31.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.language_model.model.layers.25.self_attn.k_proj.weight', 'model.language_model.model.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.language_model.model.layers.7.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.language_model.model.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.language_model.model.layers.30.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.language_model.model.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.language_model.model.layers.1.mlp.gate_proj.weight', 'model.language_model.model.layers.31.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.language_model.model.layers.27.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.language_model.model.layers.9.self_attn.v_proj.weight', 'model.language_model.model.layers.26.mlp.up_proj.weight', 'model.language_model.model.layers.3.self_attn.v_proj.weight', 'model.language_model.model.layers.30.post_attention_layernorm.weight', 'model.language_model.model.layers.15.post_attention_layernorm.weight', 'model.language_model.model.layers.19.self_attn.k_proj.weight', 'model.language_model.model.layers.28.self_attn.q_proj.weight', 'model.language_model.model.layers.7.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.language_model.model.layers.9.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.language_model.model.layers.26.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.language_model.model.layers.21.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.language_model.model.layers.21.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.language_model.model.layers.19.mlp.down_proj.weight', 'model.language_model.model.layers.10.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_model.pre_layrnorm.weight', 'model.language_model.model.layers.2.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.language_model.model.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.language_model.model.layers.29.self_attn.q_proj.weight', 'model.language_model.model.layers.27.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.language_model.model.layers.27.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.language_model.model.layers.4.post_attention_layernorm.weight', 'model.language_model.model.layers.8.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.language_model.model.layers.27.post_attention_layernorm.weight', 'model.language_model.model.layers.30.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.language_model.model.layers.4.self_attn.o_proj.weight', 'model.language_model.model.layers.30.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.language_model.model.layers.11.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.language_model.model.layers.1.self_attn.o_proj.weight', 'model.language_model.model.layers.14.input_layernorm.weight', 'model.language_model.model.layers.8.mlp.up_proj.weight', 'model.language_model.model.layers.11.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.language_model.model.layers.16.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.language_model.model.layers.0.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.language_model.model.layers.3.mlp.gate_proj.weight', 'model.language_model.lm_head.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.language_model.model.layers.30.self_attn.k_proj.weight', 'model.language_model.model.layers.27.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.language_model.model.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.language_model.model.layers.24.mlp.up_proj.weight', 'model.language_model.model.norm.weight', 'model.language_model.model.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.language_model.model.layers.2.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.language_model.model.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.language_model.model.layers.18.mlp.gate_proj.weight', 'model.language_model.model.layers.5.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.language_model.model.embed_tokens.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.language_model.model.layers.21.mlp.up_proj.weight', 'model.language_model.model.layers.20.post_attention_layernorm.weight', 'model.language_model.model.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.multi_modal_projector.linear_2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.language_model.model.layers.16.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.language_model.model.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.language_model.model.layers.23.self_attn.o_proj.weight', 'model.language_model.model.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.language_model.model.layers.18.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.language_model.model.layers.20.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.language_model.model.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.language_model.model.layers.15.self_attn.k_proj.weight', 'model.language_model.model.layers.12.mlp.up_proj.weight', 'model.language_model.model.layers.14.post_attention_layernorm.weight', 'model.language_model.model.layers.6.input_layernorm.weight', 'model.language_model.model.layers.23.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.language_model.model.layers.25.mlp.down_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.language_model.model.layers.9.post_attention_layernorm.weight', 'model.language_model.model.layers.1.self_attn.v_proj.weight', 'model.language_model.model.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.language_model.model.layers.24.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.language_model.model.layers.27.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.language_model.model.layers.2.input_layernorm.weight', 'model.language_model.model.layers.0.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.language_model.model.layers.28.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.language_model.model.layers.13.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.language_model.model.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.language_model.model.layers.26.mlp.down_proj.weight', 'model.language_model.model.layers.11.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.language_model.model.layers.14.self_attn.v_proj.weight', 'model.language_model.model.layers.25.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.language_model.model.layers.15.self_attn.v_proj.weight', 'model.language_model.model.layers.25.self_attn.q_proj.weight', 'model.language_model.model.layers.17.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.language_model.model.layers.4.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.language_model.model.layers.16.post_attention_layernorm.weight', 'model.language_model.model.layers.0.mlp.gate_proj.weight', 'model.language_model.model.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.language_model.model.layers.10.self_attn.v_proj.weight', 'model.language_model.model.layers.25.input_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.language_model.model.layers.31.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.language_model.model.layers.18.self_attn.k_proj.weight', 'model.language_model.model.layers.28.post_attention_layernorm.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.language_model.model.layers.16.mlp.up_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.language_model.model.layers.5.mlp.gate_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.multi_modal_projector.linear_1.bias', 'model.language_model.model.layers.13.input_layernorm.weight', 'model.language_model.model.layers.8.self_attn.o_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.language_model.model.layers.1.post_attention_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "generation_config.json: 100%|██████████| 154/154 [00:00<00:00, 154kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 39.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage-to-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliuhaotian/llava-v1.5-13b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:967\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    964\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    965\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 967\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:787\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    785\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m         )\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2028\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2025\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2026\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2260\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama_fast.py:124\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    113\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    123\u001b[0m ):\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[1;32mc:\\Users\\Bridget Leonard\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"liuhaotian/llava-v1.5-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "def inputs(image_folder, word_options):\n",
    "# Get images\n",
    "    images = []\n",
    "    if os.path.exists(image_folder):\n",
    "        items = os.listdir(image_folder)\n",
    "\n",
    "        for item in items:\n",
    "            item_path = os.path.join(image_folder, item)\n",
    "            images.append(Image.open(str(item_path)).convert('RGB'))\n",
    "    \n",
    "    # Get text\n",
    "    questions = {}\n",
    "    with open(word_options, 'r') as txtfile:\n",
    "\n",
    "            for i, line in enumerate(txtfile):\n",
    "                    questions[f\"Question {i+1}\"] = line.strip()\n",
    "    \n",
    "    return images, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Bridget Leonard\\\\Desktop\\\\eyes_emotion\\\\ai_results\\\\llava'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, questions = inputs('task_materials/regular', 'task_materials/wordOptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(images, questions):\n",
    "    responses = {}\n",
    "    for image in images:\n",
    "        for question, answers in questions.items():\n",
    "            prompt = f'''USER: <image>\n",
    "            Choose which word best describes what the person in the picture is thinking or feeling based \n",
    "            on just their eyes alone. \n",
    "            You may feel that more than one word is applicable, \n",
    "            but please choose just one word, the word \n",
    "            which you consider to be most suitable. \n",
    "            Your 4 choices are: {answers}\n",
    "            ASSISTANT:'''\n",
    "\n",
    "            outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "            result = [line.split(':')[-1].strip() for line in outputs[0]['generated_text'].split('\\n')][-1]\n",
    "            print(result)\n",
    "            responses[question] = result\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m(images[\u001b[38;5;241m0\u001b[39m], questions[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_model' is not defined"
     ]
    }
   ],
   "source": [
    "responses = run_model(images[0], questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
