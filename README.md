# Reading the Mind in the Eyes Test (RMET) Analysis with AI

## Introduction

This project explores the capabilities of AI models, specifically GPT-4 Vision API and LLava, in performing the Reading the Mind in the Eyes Test (RMET). RMET is a widely used tool in cognitive psychology, which assesses the ability to understand emotions by looking at a picture of eyes and selecting the emotion that best describes them from four options.

![Sample RMET Image](/task_materials/regular/01-playful-comforting-irritated-bored-300x175.jpg)
*Sample RMET Image with Options: [playful, comforting, irritated, bored]*

Our research focuses on comparing the performance of GPT-4 Vision API and LLava in this context and improving LLava's performance through fine-tuning.

## Project Description

The RMET presents a significant challenge in understanding human emotions, a task typically thought to be exclusive to human cognitive abilities. This project aims to:

1. Assess the performance of GPT-4 Vision API and LLava on the RMET.
2. Identify the impact of training dataset size on the model's performance.
3. Fine-tune the LLava model using image-caption pairs related to various emotions.
4. Evaluate the improvement in LLava's performance post fine-tuning.

Through this project, we observed a **% increase in performance in the LLava model, post fine-tuning on a dataset of *n-size*.

## Installation

To set up this project locally, follow these steps:

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
# Instructions for setting up a virtual environment (if applicable)
# Additional installation instructions
```

## Results

## Acknowledgements
