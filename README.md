# Reading the Mind in the Eyes Test (RMET) Analysis with AI

## Introduction

This project explores the capabilities of AI models, specifically GPT-4 Vision API and LLava, in performing the Reading the Mind in the Eyes Test (RMET). RMET is a widely used tool in cognitive psychology, which assesses the ability to understand emotions by looking at a picture of eyes and selecting the emotion that best describes them from four options.

<div align="center">
  <img src="/task_materials/regular/01-playful-comforting-irritated-bored-300x175.jpg" alt="Sample RMET Image"/>
  <br>
  <i>Sample RMET Image with Options: [playful, comforting, irritated, bored]</i>
  <br>
</div>

Our research focuses on comparing the performance of GPT-4 Vision API and LLava in this context and improving LLava's performance through fine-tuning.

## Project Description

The RMET presents a significant challenge in understanding human emotions, a task typically thought to be exclusive to human cognitive abilities. This project aims to:

1. Assess the performance of GPT-4 Vision API and LLava on the RMET.
2. Identify the impact of training dataset size on the model's performance.
3. Fine-tune the LLava model using image-caption pairs related to various emotions.
4. Evaluate the improvement in LLava's performance post fine-tuning.

Through this project, we observed a **% increase in performance in the LLava model, post fine-tuning on a 84 KB dataset (77 items).

## Results
### GPT-4v vs LLaVA
<div align="center">
  <br>
  <img src="/llava_hyak/output/gpt_llava_dist_overlay.png" width="800" alt="GPT-4v vs LLaVA performance on the RMET"/>
</div>

Density plots of GPT-4v and LLaVA performance overlayed on a human performance distribution. GPT-4v excels at the task, matching the higher ends of human performance while LLaVA barely meets the bottom-end of human performance.

### Fine-Tuning on Adobe Firefly-generated Images and GPT-4-generated captions
#### Fine-Tuning
To try to bridge the gap between GPT-4v and LLaVA performance, we fine-tuned the LLaVA on a dataset created using image caption pairs from Adobe Firefly and GPT-4 vision. We trained 4 different instances using different numbers of epochs:
<div align="center">
  <br>
  <img src="/llava_hyak/output/trainloss.png" width="500" alt="Training Loss over Epochs"/>
  <img src="/llava_hyak/output/evalloss.png" width="500" alt="Evaluation Loss over Epochs"/>
</div>

#### RMET Performance
We saw an increase in the performance of the fine-tuned models on the RMET task. Specifically, the models trained with 5 and 7 epochs saw a 12% increase in performance on the task.
<div align="center">
  <br>
  <img src="/llava_hyak/output/percent_increase.png" width="500" alt="Increase in performance over epochs"/>
</div>

<br>
The performance of the 5 and 7 epoch fine-tuned models often met the lower threshold of average human performance, and approached the performance of GPT-4v compared to the base model. 
<div align="center">
  <br>
  <img src="/llava_hyak/output/finetune_dist_overlay.png" width="800" alt="Comparing model performance"/>
</div>

##### Doubling the training data
We attempted to further increase LLaVA performance by providing double the training data for fine-tuning. We saw another boost of the 7-epoch model in performance. After fine-tuning, the best fine-tuned LLaVA model can now meet the lower end of average human performance.
<div align="center">
  <br>
  <img src="/llava_hyak/output/finetune_double_dist_overlay.png" width="800" alt="Comparing model performance"/>
</div>

## Conclusions
Current multi-modal models can meet human standards on emotion detection tasks. Specifically, GPT-4v meets the higher end of human performance while smaller models like LLaVA-1.5-13B reflect the lower end of human performance. This difference in model performance can likely be attributed to 1) different model architecture, 2) varying amounts of training data, or perhaps a combination of both.
To investigate if the performance difference is due to the quantity of training data, we fine-tuned LLaVA with task-relevant data generated with Adobe Firefly (images) and GPT-4v (captions). We found that fine-tuning the LLaVA with 7 epochs significantly increases the model performance to meet the lower-average range of human performance. With this increase in performance, LLaVA closely approaches the GPT-4v preview performance. Since few changes are likely made to the GPT architecture between the preview and turbo versions, the success of fine-tuning likely demonstrates that the difference in performance on the RMET is due to the quantity of training data. As such, fine-tuning efforts with more task-relevant data can likely increase LLaVA to GPT-4v standards.

## Acknowledgements
### LLaVA
[LLaVA GitHub](https://github.com/haotian-liu/LLaVA/tree/main)

- Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. (2024). LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. [Link](https://llava-vl.github.io/blog/2024-01-30-llava-next/).

- Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. (2023). Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744.

- Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. (2023). Visual Instruction Tuning. NeurIPS.

### RMET
Baron-Cohen, S., Jolliffe, T., Mortimore, C., & Robertson, M. (1997). Another advanced test of theory of mind: evidence from very high functioning adults with autism or asperger syndrome. Journal of child psychology and psychiatry, and allied disciplines, 38(7), 813â€“822. https://doi.org/10.1111/j.1469-7610.1997.tb01599.x
